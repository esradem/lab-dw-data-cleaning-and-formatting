














# importing library and read df
import pandas as pd


ins_df=pd.read_csv('https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv')
ins_df.head()


ins_df.columns = ins_df.columns.str.lower().str.replace(' ', '_')



ins_df.rename(columns={ 'st' : 'state' }, inplace=True)


ins_df.columns








ins_df['gender'].unique()


# Your code here
ins_df['gender']=ins_df['gender'].map({
    'Femal': 'F' , 
    'Male': 'M' , 
    'female' : 'F',
    'M':'M',
    'F' : 'F'
})



ins_df['state'].unique()   #ST is cities to be in one format, full name or shortcut.
ins_df['state'] = ins_df['state'].replace({
    'Cali': 'California',
    'AZ': 'Arizona',
    'WA': 'Washington'
})



ins_df['education'].unique()


ins_df['education'] = ins_df['education'].replace('Bachelors', 'Bachelor')



ins_df['customer_lifetime_value']=ins_df['customer_lifetime_value'].str.replace('%','')


ins_df['vehicle_class'].unique()



replacements={"Sports Car" : "Luxury",
"Luxury SUV" : "Luxury",
"Luxury Car" : "Luxury"}
ins_df["vehicle_class"] = ins_df["vehicle_class"].replace(replacements)














# Output: dtype('O') â†’ "object" (usually string)
ins_df['customer_lifetime_value'].dtype

#ins_df.dtypes
# Output: whole dataframes data types is listed



ins_df['customer_lifetime_value']=ins_df['customer_lifetime_value'].astype(float)



ins_df.number_of_open_complaints.unique()


ins_df['number_of_open_complaints'].str.split('/').str[1].dropna().astype(int)









# identify null values
ins_df.isnull()


# Check for null values in each column
ins_df.isna().any()





# missing_pt_rows=ins_df[ins_df['policy_type'].isnull()]
# missing_pt_rows.isnull().sum(axis=1).value_counts()
ins_df['monthly_premium_auto'].isnull().value_counts()


ins_df['income'].isna().any()


ins_df = ins_df.dropna(subset=['vehicle_class'])



missing_vc_rows=ins_df[ins_df['vehicle_class'].isnull()]
missing_vc_rows.isnull().sum(axis=1).value_counts()
ins_df['policy_type'].isnull().value_counts()
#so when I check true values are equal in each columns so i decided to drop all the rows has nan



# ins_df['total_claim_amount'].isna().value_counts()
# ins_df['total_claim_amount'].describe()

# median_value= ins_df['total_claim_amount'].median()
ins_df['total_claim_amount']= ins_df['total_claim_amount'].fillna(median_value)


#find the missing value and look if other columns are missing also
missing_state_rows = ins_df[ins_df['state'].isnull()]
missing_state_rows.isnull().sum(axis=1)
#it look like yes at the end of the dataframe. 
# ins_df['state'] = ins_df['state'].dropna()


#explorehow many nan values
ins_df['state'].isnull().value_counts()
#it is too much we need to fillit with unknow
ins_df['state'] = ins_df['state'].fillna('unknown')


#it is categorical value it iss
mode_v=ins_df['education'].mode()[0]
ins_df['education'] = ins_df.education.fillna(mode_v)


#categorical variable as a gender so fill with mode
mode_value=ins_df['gender'].mode()[0]
ins_df['gender'] = ins_df['gender'].fillna(mode_value)


#explore how many nan values
ins_df['customer'].isnull().value_counts()
#it is too much we need to fillit with unknow
ins_df['customer'] = ins_df['customer'].fillna('unknown')





ins_df['customer_lifetime_value'].describe()
#min and max has too much difference so there is skewed data we need to use median

median_v = ins_df['customer_lifetime_value'].median()
ins_df['customer_lifetime_value']= ins_df['customer_lifetime_value'].fillna(median_v)








ins_df.columns


ins_df.drop_duplicates(subset=['customer'])


#i need to keep them
ins_df['monthly_premium_auto'].duplicated().value_counts()


ins_df['monthly_premium_auto'].head()


# i need to keep them
ins_df['total_claim_amount'].duplicated()











%load_ext autoreload
%autoreload 2

from cleaning_functions import main_cleaning

# Load your data
ins_df=pd.read_csv('https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv')
# Clean your data
ins_df = main_cleaning(ins_df)











#  goal is to identify customers with a high policy claim amount and a low customer lifetime value
#total claim amount and customer lifetime value
ins_df.columns


ins_df['customer_lifetime_value'].describe()
#Low customer lifetime value; bottom 25% of the customer lifetime value


ins_df['total_claim_amount'].describe()
#high total claim amount; the top 25% of the total claim amount


# Find 75th percentile of Total Claim Amount
claim_amount_75 = ins_df['total_claim_amount'].quantile(0.75)


#25th percentile of CLV
clv_25= ins_df['customer_lifetime_value'].quantile(0.25)


#find this condition
target_customers = ins_df[
    (ins_df['total_claim_amount'] > claim_amount_75) &
    (ins_df['customer_lifetime_value'] < clv_25)
]
print(target_customers.shape)
target_customers.head()


# Step 1: Calculate thresholds
claim_amount_75 = ins_df['total_claim_amount'].quantile(0.75)
clv_25 = ins_df['customer_lifetime_value'].quantile(0.25)

# Step 2: Filter
target_customers = ins_df[
    (ins_df['total_claim_amount'] > claim_amount_75) &
    (ins_df['customer_lifetime_value'] < clv_25)
]

# Step 3: View
print(target_customers.shape)
target_customers.head()

# (Optional) Step 4: Save
target_customers.to_csv('target_customers.csv', index=False)

